{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465149cd-f249-4d74-8006-c9c1aea35e8e",
   "metadata": {},
   "source": [
    "### this notebook is for training our Agent in the lux environment and see how well we do :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fef873-ccc1-4ce2-bb74-d5a74cee1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from agent import Agent\n",
    "from network import AgentNetwork, compute_network_difference, has_converged\n",
    "from rewards import calculate_rewards\n",
    "from ac2methods import compute_advantages, compute_weight_loss, compute_action_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929f67f5-870e-4fa9-a0e1-47086019e6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-01-25 18:58:46,732:jax._src.xla_bridge:987: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# reset our gym environment\n",
    "env = LuxAIS3GymEnv(numpy_output=True)\n",
    "obs, info = env.reset()\n",
    "\n",
    "env_cfg = info[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c346dbe-184e-44ea-86c9-36f360152951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cf25d8-8d9e-462e-bacc-8bf36350b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set players\n",
    "players = {\n",
    "    \"player_0\": Agent(\"player_0\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6).to(device), device),\n",
    "    \"player_1\": Agent(\"player_1\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6).to(device), device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ecc61c-0b05-4fdc-9ba7-895cc461d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer for network\n",
    "optimizer = torch.optim.Adam(players[\"player_0\"].net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906c9da1-b84f-4932-b22c-20124a7ca50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode num: 0\n",
      "Step 0 of episode 0 completed. Loss: 3226228.0000\n",
      "Step 1 of episode 0 completed. Loss: 3343718.0000\n",
      "Step 2 of episode 0 completed. Loss: 3508213.7500\n",
      "Step 3 of episode 0 completed. Loss: 3589634.5000\n",
      "Step 4 of episode 0 completed. Loss: 3598001.5000\n",
      "Step 5 of episode 0 completed. Loss: 3556760.0000\n",
      "Step 6 of episode 0 completed. Loss: 3423549.2500\n",
      "Step 7 of episode 0 completed. Loss: 3114729.5000\n",
      "Step 8 of episode 0 completed. Loss: 2789147.5000\n",
      "Step 9 of episode 0 completed. Loss: 2757886.0000\n",
      "Step 10 of episode 0 completed. Loss: 2784425.0000\n",
      "Step 11 of episode 0 completed. Loss: 2754086.0000\n",
      "Step 12 of episode 0 completed. Loss: 2734749.0000\n",
      "Step 13 of episode 0 completed. Loss: 2802406.0000\n",
      "Step 14 of episode 0 completed. Loss: 2918467.0000\n",
      "Step 15 of episode 0 completed. Loss: 3077751.7500\n",
      "Step 16 of episode 0 completed. Loss: 3212576.0000\n",
      "Step 17 of episode 0 completed. Loss: 3247932.0000\n",
      "Step 18 of episode 0 completed. Loss: 3148130.0000\n",
      "Step 19 of episode 0 completed. Loss: 3072123.0000\n",
      "Step 20 of episode 0 completed. Loss: 3036374.5000\n",
      "Step 21 of episode 0 completed. Loss: 3063104.2500\n",
      "Step 22 of episode 0 completed. Loss: 3116654.5000\n",
      "Step 23 of episode 0 completed. Loss: 3174671.0000\n",
      "Step 24 of episode 0 completed. Loss: 3236729.5000\n",
      "Step 25 of episode 0 completed. Loss: 3284155.2500\n",
      "Step 26 of episode 0 completed. Loss: 3296159.5000\n",
      "Step 27 of episode 0 completed. Loss: 3269740.2500\n",
      "Step 28 of episode 0 completed. Loss: 3255185.5000\n",
      "Step 29 of episode 0 completed. Loss: 3280772.7500\n",
      "Step 30 of episode 0 completed. Loss: 3292475.5000\n",
      "Step 31 of episode 0 completed. Loss: 3285170.5000\n",
      "Step 32 of episode 0 completed. Loss: 3277459.0000\n",
      "Step 33 of episode 0 completed. Loss: 3235629.0000\n",
      "Step 34 of episode 0 completed. Loss: 3189248.5000\n",
      "Step 35 of episode 0 completed. Loss: 3055735.0000\n",
      "Step 36 of episode 0 completed. Loss: 3038438.0000\n",
      "Step 37 of episode 0 completed. Loss: 3031427.0000\n",
      "Step 38 of episode 0 completed. Loss: 3042487.2500\n",
      "Step 39 of episode 0 completed. Loss: 3068317.7500\n",
      "Step 40 of episode 0 completed. Loss: 3087403.5000\n",
      "Step 41 of episode 0 completed. Loss: 3116419.0000\n",
      "Step 42 of episode 0 completed. Loss: 3122469.5000\n",
      "Step 43 of episode 0 completed. Loss: 3123754.5000\n",
      "Step 44 of episode 0 completed. Loss: 3141430.5000\n",
      "Step 45 of episode 0 completed. Loss: 3154997.0000\n",
      "Step 46 of episode 0 completed. Loss: 3165613.7500\n",
      "Step 47 of episode 0 completed. Loss: 3169400.0000\n",
      "Step 48 of episode 0 completed. Loss: 3196848.0000\n",
      "Step 49 of episode 0 completed. Loss: 3232411.0000\n",
      "Step 50 of episode 0 completed. Loss: 3283409.5000\n",
      "Step 51 of episode 0 completed. Loss: 3330467.5000\n",
      "Step 52 of episode 0 completed. Loss: 3356561.5000\n",
      "Step 53 of episode 0 completed. Loss: 3383071.0000\n",
      "Step 54 of episode 0 completed. Loss: 3403674.0000\n",
      "Step 55 of episode 0 completed. Loss: 3398297.0000\n",
      "Step 56 of episode 0 completed. Loss: 3387998.2500\n",
      "Step 57 of episode 0 completed. Loss: 3367779.5000\n",
      "Step 58 of episode 0 completed. Loss: 3362511.5000\n",
      "Step 59 of episode 0 completed. Loss: 3360094.5000\n",
      "Step 60 of episode 0 completed. Loss: 3345598.5000\n",
      "Step 61 of episode 0 completed. Loss: 3323278.5000\n",
      "Step 62 of episode 0 completed. Loss: 3291672.0000\n",
      "Step 63 of episode 0 completed. Loss: 3267453.0000\n",
      "Step 64 of episode 0 completed. Loss: 3247444.5000\n",
      "Step 65 of episode 0 completed. Loss: 3224548.5000\n",
      "Step 66 of episode 0 completed. Loss: 3194597.0000\n",
      "Step 67 of episode 0 completed. Loss: 3162570.5000\n",
      "Step 68 of episode 0 completed. Loss: 3210209.0000\n",
      "Step 69 of episode 0 completed. Loss: 3187741.0000\n",
      "Step 70 of episode 0 completed. Loss: 3161321.5000\n",
      "Step 71 of episode 0 completed. Loss: 3129697.0000\n",
      "Step 72 of episode 0 completed. Loss: 3097110.0000\n",
      "Step 73 of episode 0 completed. Loss: 3063838.7500\n",
      "Step 74 of episode 0 completed. Loss: 3030232.5000\n",
      "Step 75 of episode 0 completed. Loss: 2996609.0000\n",
      "Step 76 of episode 0 completed. Loss: 2968640.0000\n",
      "Step 77 of episode 0 completed. Loss: 2941877.0000\n",
      "Step 78 of episode 0 completed. Loss: 2915108.5000\n",
      "Step 79 of episode 0 completed. Loss: 2888526.5000\n",
      "Step 80 of episode 0 completed. Loss: 2862200.2500\n",
      "Step 81 of episode 0 completed. Loss: 2836317.2500\n",
      "Step 82 of episode 0 completed. Loss: 2810969.5000\n",
      "Step 83 of episode 0 completed. Loss: 2786034.0000\n",
      "Step 84 of episode 0 completed. Loss: 2761181.2500\n",
      "Step 85 of episode 0 completed. Loss: 2735688.2500\n",
      "Step 86 of episode 0 completed. Loss: 2711952.5000\n",
      "Step 87 of episode 0 completed. Loss: 2689884.0000\n",
      "Step 88 of episode 0 completed. Loss: 2668891.0000\n",
      "Step 89 of episode 0 completed. Loss: 2648890.7500\n",
      "Step 90 of episode 0 completed. Loss: 2629849.0000\n",
      "Step 91 of episode 0 completed. Loss: 2611720.5000\n",
      "Step 92 of episode 0 completed. Loss: 2594357.0000\n",
      "Step 93 of episode 0 completed. Loss: 2577815.0000\n",
      "Step 94 of episode 0 completed. Loss: 2562179.0000\n",
      "Step 95 of episode 0 completed. Loss: 2547336.5000\n",
      "Step 96 of episode 0 completed. Loss: 2533234.5000\n",
      "Step 97 of episode 0 completed. Loss: 2519815.2500\n",
      "Step 98 of episode 0 completed. Loss: 2507034.7500\n",
      "Step 99 of episode 0 completed. Loss: 2494834.5000\n",
      "Step 100 of episode 0 completed. Loss: 2177664.5000\n",
      "Step 101 of episode 0 completed. Loss: 2474852.0000\n",
      "Step 102 of episode 0 completed. Loss: 2463395.5000\n",
      "Step 103 of episode 0 completed. Loss: 2449019.5000\n",
      "Step 104 of episode 0 completed. Loss: 2442258.5000\n",
      "Step 105 of episode 0 completed. Loss: 2431971.0000\n",
      "Step 106 of episode 0 completed. Loss: 2421689.0000\n",
      "Step 107 of episode 0 completed. Loss: 2415973.0000\n",
      "Step 108 of episode 0 completed. Loss: 2406602.0000\n",
      "Step 109 of episode 0 completed. Loss: 2397487.0000\n",
      "Step 110 of episode 0 completed. Loss: 2384823.0000\n",
      "Step 111 of episode 0 completed. Loss: 2376532.0000\n",
      "Step 112 of episode 0 completed. Loss: 2368555.0000\n",
      "Step 113 of episode 0 completed. Loss: 2363856.0000\n",
      "Step 114 of episode 0 completed. Loss: 2356201.5000\n",
      "Step 115 of episode 0 completed. Loss: 2348814.7500\n",
      "Step 116 of episode 0 completed. Loss: 2344248.0000\n",
      "Step 117 of episode 0 completed. Loss: 2337089.5000\n",
      "Step 118 of episode 0 completed. Loss: 2330224.0000\n",
      "Step 119 of episode 0 completed. Loss: 2325889.0000\n",
      "Step 120 of episode 0 completed. Loss: 2319278.0000\n",
      "Step 121 of episode 0 completed. Loss: 2312895.0000\n",
      "Step 122 of episode 0 completed. Loss: 2305467.5000\n",
      "Step 123 of episode 0 completed. Loss: 2299410.0000\n",
      "Step 124 of episode 0 completed. Loss: 2293532.0000\n",
      "Step 125 of episode 0 completed. Loss: 2289593.0000\n",
      "Step 126 of episode 0 completed. Loss: 2283844.2500\n",
      "Step 127 of episode 0 completed. Loss: 2278247.2500\n",
      "Step 128 of episode 0 completed. Loss: 2274379.2500\n",
      "Step 129 of episode 0 completed. Loss: 2268899.0000\n",
      "Step 130 of episode 0 completed. Loss: 2263601.5000\n",
      "Step 131 of episode 0 completed. Loss: 2257462.0000\n",
      "Step 132 of episode 0 completed. Loss: 2252182.5000\n",
      "Step 133 of episode 0 completed. Loss: 2247172.5000\n",
      "Step 134 of episode 0 completed. Loss: 2243330.5000\n",
      "Step 135 of episode 0 completed. Loss: 2239888.7500\n",
      "Step 136 of episode 0 completed. Loss: 2236160.0000\n",
      "Step 137 of episode 0 completed. Loss: 2233279.7500\n",
      "Step 138 of episode 0 completed. Loss: 2229523.5000\n",
      "Step 139 of episode 0 completed. Loss: 2225723.5000\n",
      "Step 140 of episode 0 completed. Loss: 2222497.7500\n",
      "Step 141 of episode 0 completed. Loss: 2218669.0000\n",
      "Step 142 of episode 0 completed. Loss: 2214797.2500\n",
      "Step 143 of episode 0 completed. Loss: 2210886.5000\n",
      "Step 144 of episode 0 completed. Loss: 2207001.0000\n",
      "Step 145 of episode 0 completed. Loss: 2203074.5000\n",
      "Step 146 of episode 0 completed. Loss: 2199108.5000\n",
      "Step 147 of episode 0 completed. Loss: 2195163.5000\n",
      "Step 148 of episode 0 completed. Loss: 2191178.0000\n",
      "Step 149 of episode 0 completed. Loss: 2187153.5000\n",
      "Step 150 of episode 0 completed. Loss: 2183091.5000\n",
      "Step 151 of episode 0 completed. Loss: 2178992.5000\n",
      "Step 152 of episode 0 completed. Loss: 2174859.0000\n",
      "Step 153 of episode 0 completed. Loss: 2170690.5000\n",
      "Step 154 of episode 0 completed. Loss: 2166488.5000\n",
      "Step 155 of episode 0 completed. Loss: 2162254.2500\n",
      "Step 156 of episode 0 completed. Loss: 2157986.5000\n",
      "Step 157 of episode 0 completed. Loss: 2153687.7500\n",
      "Step 158 of episode 0 completed. Loss: 2149358.7500\n",
      "Step 159 of episode 0 completed. Loss: 2144999.5000\n",
      "Step 160 of episode 0 completed. Loss: 2140612.5000\n",
      "Step 161 of episode 0 completed. Loss: 2136196.2500\n",
      "Step 162 of episode 0 completed. Loss: 2131751.5000\n",
      "Step 163 of episode 0 completed. Loss: 2127279.5000\n",
      "Step 164 of episode 0 completed. Loss: 2122780.5000\n",
      "Step 165 of episode 0 completed. Loss: 2118254.2500\n",
      "Step 166 of episode 0 completed. Loss: 2113701.7500\n",
      "Step 167 of episode 0 completed. Loss: 2109123.5000\n",
      "Step 168 of episode 0 completed. Loss: 2103428.0000\n",
      "Step 169 of episode 0 completed. Loss: 2097848.5000\n",
      "Step 170 of episode 0 completed. Loss: 2092289.7500\n",
      "Step 171 of episode 0 completed. Loss: 2086751.5000\n",
      "Step 172 of episode 0 completed. Loss: 2081230.2500\n",
      "Step 173 of episode 0 completed. Loss: 2075725.8750\n",
      "Step 174 of episode 0 completed. Loss: 2070235.5000\n",
      "Step 175 of episode 0 completed. Loss: 2064758.5000\n",
      "Step 176 of episode 0 completed. Loss: 2059293.0000\n",
      "Step 177 of episode 0 completed. Loss: 2053838.7500\n",
      "Step 178 of episode 0 completed. Loss: 2048393.2500\n",
      "Step 179 of episode 0 completed. Loss: 2042955.0000\n",
      "Step 180 of episode 0 completed. Loss: 2037523.7500\n",
      "Step 181 of episode 0 completed. Loss: 2032098.6250\n",
      "Step 182 of episode 0 completed. Loss: 2026677.5000\n",
      "Step 183 of episode 0 completed. Loss: 2021259.7500\n",
      "Step 184 of episode 0 completed. Loss: 2015845.6250\n",
      "Step 185 of episode 0 completed. Loss: 2010431.6250\n",
      "Step 186 of episode 0 completed. Loss: 2005018.7500\n",
      "Step 187 of episode 0 completed. Loss: 1999606.2500\n",
      "Step 188 of episode 0 completed. Loss: 1994193.0000\n",
      "Step 189 of episode 0 completed. Loss: 1988777.0000\n",
      "Step 190 of episode 0 completed. Loss: 1983358.7500\n",
      "Step 191 of episode 0 completed. Loss: 1977937.3750\n",
      "Step 192 of episode 0 completed. Loss: 1972512.0000\n",
      "Step 193 of episode 0 completed. Loss: 1967082.1250\n",
      "Step 194 of episode 0 completed. Loss: 1961646.7500\n",
      "Step 195 of episode 0 completed. Loss: 1956205.2500\n",
      "Step 196 of episode 0 completed. Loss: 1950758.0000\n",
      "Step 197 of episode 0 completed. Loss: 1945303.5000\n",
      "Step 198 of episode 0 completed. Loss: 1939840.7500\n",
      "Step 199 of episode 0 completed. Loss: 1934370.2500\n",
      "Step 200 of episode 0 completed. Loss: 1928891.7500\n",
      "Step 201 of episode 0 completed. Loss: 1653378.5000\n",
      "Step 202 of episode 0 completed. Loss: 1918329.5000\n",
      "Step 203 of episode 0 completed. Loss: 1913041.7500\n",
      "Step 204 of episode 0 completed. Loss: 1907732.0000\n",
      "Step 205 of episode 0 completed. Loss: 1902557.7500\n",
      "Step 206 of episode 0 completed. Loss: 1897191.7500\n",
      "Step 207 of episode 0 completed. Loss: 1891805.2500\n",
      "Step 208 of episode 0 completed. Loss: 1886545.2500\n",
      "Step 209 of episode 0 completed. Loss: 1881104.2500\n",
      "Step 210 of episode 0 completed. Loss: 1875643.3750\n",
      "Step 211 of episode 0 completed. Loss: 1870302.0000\n",
      "Step 212 of episode 0 completed. Loss: 1864788.3750\n",
      "Step 213 of episode 0 completed. Loss: 1859256.0000\n",
      "Step 214 of episode 0 completed. Loss: 1853835.5000\n",
      "Step 215 of episode 0 completed. Loss: 1848252.3750\n",
      "Step 216 of episode 0 completed. Loss: 1842650.3750\n",
      "Step 217 of episode 0 completed. Loss: 1837153.5000\n",
      "Step 218 of episode 0 completed. Loss: 1831501.7500\n",
      "Step 219 of episode 0 completed. Loss: 1825835.3750\n",
      "Step 220 of episode 0 completed. Loss: 1820257.3750\n",
      "Step 221 of episode 0 completed. Loss: 1814548.5000\n",
      "Step 222 of episode 0 completed. Loss: 1808824.1250\n",
      "Step 223 of episode 0 completed. Loss: 1803155.5000\n",
      "Step 224 of episode 0 completed. Loss: 1797388.3750\n",
      "Step 225 of episode 0 completed. Loss: 1791606.2500\n",
      "Step 226 of episode 0 completed. Loss: 1785849.5000\n",
      "Step 227 of episode 0 completed. Loss: 1780025.3750\n",
      "Step 228 of episode 0 completed. Loss: 1774184.7500\n",
      "Step 229 of episode 0 completed. Loss: 1768343.7500\n",
      "Step 230 of episode 0 completed. Loss: 1762461.5000\n",
      "Step 231 of episode 0 completed. Loss: 1756563.5000\n",
      "Step 232 of episode 0 completed. Loss: 1750639.2500\n",
      "Step 233 of episode 0 completed. Loss: 1744699.7500\n",
      "Step 234 of episode 0 completed. Loss: 1738742.7500\n",
      "Step 235 of episode 0 completed. Loss: 1732894.0000\n",
      "Step 236 of episode 0 completed. Loss: 1727038.2500\n",
      "Step 237 of episode 0 completed. Loss: 1721157.5000\n",
      "Step 238 of episode 0 completed. Loss: 1715246.2500\n",
      "Step 239 of episode 0 completed. Loss: 1709324.8750\n",
      "Step 240 of episode 0 completed. Loss: 1703378.3750\n",
      "Step 241 of episode 0 completed. Loss: 1697424.0000\n",
      "Step 242 of episode 0 completed. Loss: 1691436.0000\n",
      "Step 243 of episode 0 completed. Loss: 1685422.7500\n",
      "Step 244 of episode 0 completed. Loss: 1679422.8750\n",
      "Step 245 of episode 0 completed. Loss: 1673367.6250\n",
      "Step 246 of episode 0 completed. Loss: 1667287.0000\n",
      "Step 247 of episode 0 completed. Loss: 1661238.7500\n",
      "Step 248 of episode 0 completed. Loss: 1655114.5000\n",
      "Step 249 of episode 0 completed. Loss: 1648965.7500\n",
      "Step 250 of episode 0 completed. Loss: 1642791.5000\n",
      "Step 251 of episode 0 completed. Loss: 1636593.0000\n",
      "Step 252 of episode 0 completed. Loss: 1630370.5000\n",
      "Step 253 of episode 0 completed. Loss: 1624123.0000\n",
      "Step 254 of episode 0 completed. Loss: 1617850.2500\n",
      "Step 255 of episode 0 completed. Loss: 1611553.7500\n",
      "Step 256 of episode 0 completed. Loss: 1605233.0000\n",
      "Step 257 of episode 0 completed. Loss: 1598888.3750\n",
      "Step 258 of episode 0 completed. Loss: 1592518.2500\n",
      "Step 259 of episode 0 completed. Loss: 1586124.2500\n",
      "Step 260 of episode 0 completed. Loss: 1579706.0000\n",
      "Step 261 of episode 0 completed. Loss: 1573264.0000\n",
      "Step 262 of episode 0 completed. Loss: 1566798.2500\n",
      "Step 263 of episode 0 completed. Loss: 1560308.1250\n",
      "Step 264 of episode 0 completed. Loss: 1553793.2500\n",
      "Step 265 of episode 0 completed. Loss: 1547255.2500\n",
      "Step 266 of episode 0 completed. Loss: 1540693.1250\n",
      "Step 267 of episode 0 completed. Loss: 1534107.5000\n",
      "Step 268 of episode 0 completed. Loss: 1527418.5000\n",
      "Step 269 of episode 0 completed. Loss: 1520707.5000\n",
      "Step 270 of episode 0 completed. Loss: 1513976.2500\n",
      "Step 271 of episode 0 completed. Loss: 1507222.0000\n",
      "Step 272 of episode 0 completed. Loss: 1500447.7500\n",
      "Step 273 of episode 0 completed. Loss: 1493651.7500\n",
      "Step 274 of episode 0 completed. Loss: 1486834.5000\n",
      "Step 275 of episode 0 completed. Loss: 1479995.6250\n",
      "Step 276 of episode 0 completed. Loss: 1473135.5000\n",
      "Step 277 of episode 0 completed. Loss: 1466254.0000\n",
      "Step 278 of episode 0 completed. Loss: 1459350.7500\n",
      "Step 279 of episode 0 completed. Loss: 1452426.2500\n",
      "Step 280 of episode 0 completed. Loss: 1445480.3750\n",
      "Step 281 of episode 0 completed. Loss: 1438512.8750\n",
      "Step 282 of episode 0 completed. Loss: 1431523.6250\n",
      "Step 283 of episode 0 completed. Loss: 1424512.2500\n",
      "Step 284 of episode 0 completed. Loss: 1417481.2500\n",
      "Step 285 of episode 0 completed. Loss: 1410426.7500\n",
      "Step 286 of episode 0 completed. Loss: 1403351.0000\n",
      "Step 287 of episode 0 completed. Loss: 1396254.2500\n",
      "Step 288 of episode 0 completed. Loss: 1389135.7500\n",
      "Step 289 of episode 0 completed. Loss: 1381995.5000\n",
      "Step 290 of episode 0 completed. Loss: 1374832.7500\n",
      "Step 291 of episode 0 completed. Loss: 1367649.1250\n",
      "Step 292 of episode 0 completed. Loss: 1360443.7500\n",
      "Step 293 of episode 0 completed. Loss: 1353215.3750\n",
      "Step 294 of episode 0 completed. Loss: 1345966.7500\n",
      "Step 295 of episode 0 completed. Loss: 1338695.5000\n",
      "Step 296 of episode 0 completed. Loss: 1331403.0000\n",
      "Step 297 of episode 0 completed. Loss: 1324088.3750\n",
      "Step 298 of episode 0 completed. Loss: 1316752.1250\n",
      "Step 299 of episode 0 completed. Loss: 1309394.1250\n",
      "Step 300 of episode 0 completed. Loss: 1302013.0000\n",
      "Step 301 of episode 0 completed. Loss: 1294654.3750\n",
      "Step 302 of episode 0 completed. Loss: 1057357.6250\n",
      "Step 303 of episode 0 completed. Loss: 1279801.5000\n",
      "Step 304 of episode 0 completed. Loss: 1272373.7500\n",
      "Step 305 of episode 0 completed. Loss: 1264920.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_, agent \u001b[38;5;129;01min\u001b[39;00m players\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     45\u001b[0m     network_outs[id_] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact_train(step\u001b[38;5;241m=\u001b[39mstep, obs\u001b[38;5;241m=\u001b[39mobs[id_])\n\u001b[0;32m---> 47\u001b[0m     actions[id_] \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# save actions\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     last_actions[id_] \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/lux_s3_submission/agent.py:184\u001b[0m, in \u001b[0;36mAgent.sample_actions\u001b[0;34m(self, action_probs, sap_offset)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Apply dx, dy only for sap actions\u001b[39;00m\n\u001b[1;32m    183\u001b[0m sap_action_indices \u001b[38;5;241m=\u001b[39m (action_indices \u001b[38;5;241m==\u001b[39m action_space \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Assuming sap is the last action\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m sampled_dx[sap_action_indices] \u001b[38;5;241m=\u001b[39m \u001b[43mdx\u001b[49m\u001b[43m[\u001b[49m\u001b[43msap_action_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    185\u001b[0m sampled_dy[sap_action_indices] \u001b[38;5;241m=\u001b[39m dy[sap_action_indices]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Combine into final output (batch_size, 3)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# set some hyperparams\n",
    "episode_num = 0\n",
    "reward_history = []\n",
    "network_difs = []\n",
    "wins = 0\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "value_coeff=0.5\n",
    "entropy_coeff=0.01\n",
    "win_rates = []\n",
    "\n",
    "while True:\n",
    "    obs, info = env.reset()\n",
    "    game_done = False\n",
    "    wins = 0\n",
    "    step = 0\n",
    "    last_obs = {}\n",
    "    last_actions = {}\n",
    "    print(f\"episode num: {episode_num}\")\n",
    "\n",
    "    # initialize rewards array and trajectories\n",
    "    rewards = {\n",
    "        \"player_0\": [],\n",
    "        \"player_1\": []\n",
    "    }\n",
    "\n",
    "    # save last env reward \n",
    "    last_env_reward = {\n",
    "        \"player_0\": np.zeros(1, dtype=np.int32),\n",
    "        \"player_1\": np.zeros(1, dtype=np.int32)\n",
    "    }\n",
    "    \n",
    "    while not game_done:\n",
    "        actions = {}\n",
    "        # store current observations for learning\n",
    "        last_obs = {\n",
    "            \"player_0\": obs[\"player_0\"].copy(),\n",
    "            \"player_1\": obs[\"player_1\"].copy()\n",
    "        }\n",
    "\n",
    "        # get network output, including actions\n",
    "        network_outs = {}\n",
    "        for id_, agent in players.items():\n",
    "            \n",
    "            network_outs[id_] = agent.act_train(step=step, obs=obs[id_])\n",
    "\n",
    "            actions[id_] = agent.sample_actions(network_outs[id_][1].detach().cpu(), network_outs[id_][2].detach().cpu())\n",
    "\n",
    "            # save actions\n",
    "            last_actions[id_] = actions.copy()\n",
    "\n",
    "         \n",
    "        # step in environment for both agents\n",
    "        obs, reward, terminated, truncated, info = env.step(actions)\n",
    "        match_result = None\n",
    "        if (last_env_reward != reward):\n",
    "            if reward[\"player_0\"] > last_env_reward[\"player_0\"]:\n",
    "                match_result = \"win\"\n",
    "            elif reward[\"player_1\"] > last_env_reward[\"player_1\"]:\n",
    "                match_result = \"loss\"\n",
    "            else:\n",
    "                match_result = \"draw\"\n",
    "\n",
    "        last_env_reward = reward.copy()\n",
    "\n",
    "        # calc rewards for both agents\n",
    "        for id_, agent in players.items():\n",
    "            map_memory, enemy_memory, ally_memory, relic_points, _, _ = agent.process_obs(obs[id_])\n",
    "            rewards[id_].append(calculate_rewards(network_outs[id_][0].squeeze(0).detach().cpu().numpy(), map_memory, enemy_memory, ally_memory, relic_points, match_result))\n",
    "            \n",
    "\n",
    "        # calc whether game is finished\n",
    "        dones = {k: terminated[k] | truncated[k] for k in terminated}\n",
    "\n",
    "        # Compute returns and advantages for player 0\n",
    "        returns, advantages = compute_advantages(\n",
    "            rewards=[rewards[\"player_0\"][-1]],\n",
    "            values=[network_outs[\"player_0\"][3].squeeze(0).squeeze(-1).detach().cpu().numpy()],\n",
    "            gamma=gamma,\n",
    "            lambda_=lambda_\n",
    "        )\n",
    "\n",
    "        # compute losses\n",
    "        weight_loss = compute_weight_loss(\n",
    "            log_probs=torch.cat((network_outs[\"player_0\"][1].log(), network_outs[\"player_0\"][2].log()), dim=-1).to(device),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "        action_loss = compute_action_loss(\n",
    "            log_probs=network_outs[\"player_0\"][1].log(),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "    \n",
    "        # backpropogation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = weight_loss + action_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Step {step} of episode {episode_num} completed. Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        if dones[\"player_0\"] or dones[\"player_1\"]:\n",
    "            game_done = True\n",
    "            # save model weights\n",
    "            torch.save(players[\"player_0\"].net.state_dict(), f\"models/agent_network_episode_{episode_num}\")\n",
    "            wins += int(reward[\"player_0\"] > reward[\"player_1\"])\n",
    "            print(wins)\n",
    "            win_rates.append(wins / (episode_num + 1))\n",
    "        step += 1\n",
    "\n",
    "    # store rewards\n",
    "    reward_history.append(rewards[\"player_0\"])\n",
    "   \n",
    "\n",
    "    # calc l2 norm\n",
    "    network_dif = compute_network_difference(players[\"player_0\"].net, players[\"player_1\"].net)\n",
    "    network_difs.append(network_dif)\n",
    "\n",
    "    # update adversary to current state dict every 5 episodes\n",
    "    if episode_num % 5 == 0:\n",
    "        players[\"player_1\"].net.load_state_dict(players[\"player_0\"].net.state_dict())\n",
    "\n",
    "    # if network has converged according to our criterion break out of training loop\n",
    "    if has_converged(win_rates, network_difs):\n",
    "        break\n",
    "        \n",
    "    episode_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679f8e2-5a1a-479c-9d9c-f4c1296f2b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a2aa4-65aa-4b98-b4ac-f21f09918777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db2411-e81f-47f9-91a9-b0615f95e531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
