{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465149cd-f249-4d74-8006-c9c1aea35e8e",
   "metadata": {},
   "source": [
    "### this notebook is for training our Agent in the lux environment and see how well we do :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fef873-ccc1-4ce2-bb74-d5a74cee1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from agent import Agent\n",
    "from network import AgentNetwork, compute_network_difference, has_converged\n",
    "from rewards import calculate_rewards\n",
    "from ac2methods import compute_advantages, compute_weight_loss, compute_action_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929f67f5-870e-4fa9-a0e1-47086019e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset our gym environment\n",
    "env = LuxAIS3GymEnv(numpy_output=True)\n",
    "obs, info = env.reset()\n",
    "\n",
    "env_cfg = info[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c346dbe-184e-44ea-86c9-36f360152951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cf25d8-8d9e-462e-bacc-8bf36350b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set players\n",
    "players = {\n",
    "    \"player_0\": Agent(\"player_0\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6), device),\n",
    "    \"player_1\": Agent(\"player_1\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6), device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ecc61c-0b05-4fdc-9ba7-895cc461d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer for network\n",
    "optimizer = torch.optim.Adam(players[\"player_0\"].net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906c9da1-b84f-4932-b22c-20124a7ca50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode num: 0\n",
      "Step 0 of episode 0 completed. Loss: 4492067.0000\n",
      "Step 1 of episode 0 completed. Loss: 4329886.5000\n",
      "Step 2 of episode 0 completed. Loss: 4209253.5000\n",
      "Step 3 of episode 0 completed. Loss: 4067008.5000\n",
      "Step 4 of episode 0 completed. Loss: 3950089.0000\n",
      "Step 5 of episode 0 completed. Loss: 3843604.2500\n",
      "Step 6 of episode 0 completed. Loss: 3748560.0000\n",
      "Step 7 of episode 0 completed. Loss: 3680598.2500\n",
      "Step 8 of episode 0 completed. Loss: 3637161.0000\n",
      "Step 9 of episode 0 completed. Loss: 3610538.5000\n",
      "Step 10 of episode 0 completed. Loss: 3566683.7500\n",
      "Step 11 of episode 0 completed. Loss: 3541881.5000\n",
      "Step 12 of episode 0 completed. Loss: 3552720.0000\n",
      "Step 13 of episode 0 completed. Loss: 3581873.0000\n",
      "Step 14 of episode 0 completed. Loss: 3590470.5000\n",
      "Step 15 of episode 0 completed. Loss: 3559179.2500\n",
      "Step 16 of episode 0 completed. Loss: 3502811.0000\n",
      "Step 17 of episode 0 completed. Loss: 3448052.7500\n",
      "Step 18 of episode 0 completed. Loss: 3370401.0000\n",
      "Step 19 of episode 0 completed. Loss: 3289069.5000\n",
      "Step 20 of episode 0 completed. Loss: 3192319.0000\n",
      "Step 21 of episode 0 completed. Loss: 3097765.2500\n",
      "Step 22 of episode 0 completed. Loss: 3054841.5000\n",
      "Step 23 of episode 0 completed. Loss: 3045467.2500\n",
      "Step 24 of episode 0 completed. Loss: 3056752.5000\n",
      "Step 25 of episode 0 completed. Loss: 3073722.5000\n",
      "Step 26 of episode 0 completed. Loss: 3081002.7500\n",
      "Step 27 of episode 0 completed. Loss: 3089108.0000\n",
      "Step 28 of episode 0 completed. Loss: 3110455.0000\n",
      "Step 29 of episode 0 completed. Loss: 3132397.5000\n",
      "Step 30 of episode 0 completed. Loss: 3136583.2500\n",
      "Step 31 of episode 0 completed. Loss: 3131155.0000\n",
      "Step 32 of episode 0 completed. Loss: 3133136.5000\n",
      "Step 33 of episode 0 completed. Loss: 3126316.0000\n",
      "Step 34 of episode 0 completed. Loss: 3100692.0000\n",
      "Step 35 of episode 0 completed. Loss: 3093547.0000\n",
      "Step 36 of episode 0 completed. Loss: 3104108.0000\n",
      "Step 37 of episode 0 completed. Loss: 3114724.0000\n",
      "Step 38 of episode 0 completed. Loss: 3119222.2500\n",
      "Step 39 of episode 0 completed. Loss: 3118188.0000\n",
      "Step 40 of episode 0 completed. Loss: 3113905.5000\n",
      "Step 41 of episode 0 completed. Loss: 3094230.5000\n",
      "Step 42 of episode 0 completed. Loss: 3081281.5000\n",
      "Step 43 of episode 0 completed. Loss: 3068279.2500\n",
      "Step 44 of episode 0 completed. Loss: 3062993.5000\n",
      "Step 45 of episode 0 completed. Loss: 3069617.5000\n",
      "Step 46 of episode 0 completed. Loss: 3087230.0000\n",
      "Step 47 of episode 0 completed. Loss: 3096992.7500\n",
      "Step 48 of episode 0 completed. Loss: 3096837.5000\n",
      "Step 49 of episode 0 completed. Loss: 3091809.0000\n",
      "Step 50 of episode 0 completed. Loss: 3081517.0000\n",
      "Step 51 of episode 0 completed. Loss: 3067689.7500\n",
      "Step 52 of episode 0 completed. Loss: 3054916.0000\n",
      "Step 53 of episode 0 completed. Loss: 3044690.5000\n",
      "Step 54 of episode 0 completed. Loss: 3032482.2500\n",
      "Step 55 of episode 0 completed. Loss: 3018490.2500\n",
      "Step 56 of episode 0 completed. Loss: 3002306.0000\n",
      "Step 57 of episode 0 completed. Loss: 2984069.5000\n",
      "Step 58 of episode 0 completed. Loss: 2963775.5000\n",
      "Step 59 of episode 0 completed. Loss: 2941160.2500\n",
      "Step 60 of episode 0 completed. Loss: 2913631.5000\n",
      "Step 61 of episode 0 completed. Loss: 2888753.2500\n",
      "Step 62 of episode 0 completed. Loss: 2863805.0000\n",
      "Step 63 of episode 0 completed. Loss: 2834581.5000\n",
      "Step 64 of episode 0 completed. Loss: 2796356.5000\n",
      "Step 65 of episode 0 completed. Loss: 2757302.5000\n",
      "Step 66 of episode 0 completed. Loss: 2724869.0000\n",
      "Step 67 of episode 0 completed. Loss: 2705837.5000\n",
      "Step 68 of episode 0 completed. Loss: 2690463.5000\n",
      "Step 69 of episode 0 completed. Loss: 2672001.0000\n",
      "Step 70 of episode 0 completed. Loss: 2663711.0000\n",
      "Step 71 of episode 0 completed. Loss: 2655936.0000\n",
      "Step 72 of episode 0 completed. Loss: 2648091.5000\n",
      "Step 73 of episode 0 completed. Loss: 2641629.0000\n",
      "Step 74 of episode 0 completed. Loss: 2630512.7500\n",
      "Step 75 of episode 0 completed. Loss: 2617395.0000\n",
      "Step 76 of episode 0 completed. Loss: 2604146.5000\n",
      "Step 77 of episode 0 completed. Loss: 2591132.0000\n",
      "Step 78 of episode 0 completed. Loss: 2578527.7500\n",
      "Step 79 of episode 0 completed. Loss: 2566192.0000\n",
      "Step 80 of episode 0 completed. Loss: 2554208.0000\n",
      "Step 81 of episode 0 completed. Loss: 2541760.0000\n",
      "Step 82 of episode 0 completed. Loss: 2528690.2500\n",
      "Step 83 of episode 0 completed. Loss: 2515880.0000\n",
      "Step 84 of episode 0 completed. Loss: 2503655.5000\n",
      "Step 85 of episode 0 completed. Loss: 2492335.7500\n",
      "Step 86 of episode 0 completed. Loss: 2481092.7500\n",
      "Step 87 of episode 0 completed. Loss: 2470575.0000\n",
      "Step 88 of episode 0 completed. Loss: 2460826.2500\n",
      "Step 89 of episode 0 completed. Loss: 2450774.5000\n",
      "Step 90 of episode 0 completed. Loss: 2441318.2500\n",
      "Step 91 of episode 0 completed. Loss: 2433891.5000\n",
      "Step 92 of episode 0 completed. Loss: 2426528.0000\n",
      "Step 93 of episode 0 completed. Loss: 2419440.2500\n",
      "Step 94 of episode 0 completed. Loss: 2412580.2500\n",
      "Step 95 of episode 0 completed. Loss: 2405932.5000\n",
      "Step 96 of episode 0 completed. Loss: 2399483.0000\n",
      "Step 97 of episode 0 completed. Loss: 2393185.7500\n",
      "Step 98 of episode 0 completed. Loss: 2387064.0000\n",
      "Step 99 of episode 0 completed. Loss: 2381105.0000\n",
      "Step 100 of episode 0 completed. Loss: 2531459.2500\n",
      "Step 101 of episode 0 completed. Loss: 2366275.2500\n",
      "Step 102 of episode 0 completed. Loss: 2365976.5000\n",
      "Step 103 of episode 0 completed. Loss: 2360638.7500\n",
      "Step 104 of episode 0 completed. Loss: 2353772.0000\n",
      "Step 105 of episode 0 completed. Loss: 2350949.2500\n",
      "Step 106 of episode 0 completed. Loss: 2346153.0000\n",
      "Step 107 of episode 0 completed. Loss: 2338569.5000\n",
      "Step 108 of episode 0 completed. Loss: 2336379.7500\n",
      "Step 109 of episode 0 completed. Loss: 2333645.5000\n",
      "Step 110 of episode 0 completed. Loss: 2326174.0000\n",
      "Step 111 of episode 0 completed. Loss: 2322722.5000\n",
      "Step 112 of episode 0 completed. Loss: 2316405.7500\n",
      "Step 113 of episode 0 completed. Loss: 2308629.5000\n",
      "Step 114 of episode 0 completed. Loss: 2304266.0000\n",
      "Step 115 of episode 0 completed. Loss: 2299057.0000\n",
      "Step 116 of episode 0 completed. Loss: 2293341.5000\n",
      "Step 117 of episode 0 completed. Loss: 2290062.0000\n",
      "Step 118 of episode 0 completed. Loss: 2284307.5000\n",
      "Step 119 of episode 0 completed. Loss: 2277940.5000\n",
      "Step 120 of episode 0 completed. Loss: 2275159.0000\n",
      "Step 121 of episode 0 completed. Loss: 2268657.0000\n",
      "Step 122 of episode 0 completed. Loss: 2263043.0000\n",
      "Step 123 of episode 0 completed. Loss: 2259826.0000\n",
      "Step 124 of episode 0 completed. Loss: 2254463.5000\n",
      "Step 125 of episode 0 completed. Loss: 2248865.0000\n",
      "Step 126 of episode 0 completed. Loss: 2245402.5000\n",
      "Step 127 of episode 0 completed. Loss: 2240311.0000\n",
      "Step 128 of episode 0 completed. Loss: 2234396.5000\n",
      "Step 129 of episode 0 completed. Loss: 2229645.5000\n",
      "Step 130 of episode 0 completed. Loss: 2224553.0000\n",
      "Step 131 of episode 0 completed. Loss: 2219319.0000\n",
      "Step 132 of episode 0 completed. Loss: 2215051.0000\n",
      "Step 133 of episode 0 completed. Loss: 2209066.5000\n",
      "Step 134 of episode 0 completed. Loss: 2202995.2500\n",
      "Step 135 of episode 0 completed. Loss: 2197855.0000\n",
      "Step 136 of episode 0 completed. Loss: 2192673.5000\n",
      "Step 137 of episode 0 completed. Loss: 2186354.2500\n",
      "Step 138 of episode 0 completed. Loss: 2181853.5000\n",
      "Step 139 of episode 0 completed. Loss: 2177156.5000\n",
      "Step 140 of episode 0 completed. Loss: 2172065.7500\n",
      "Step 141 of episode 0 completed. Loss: 2167584.5000\n",
      "Step 142 of episode 0 completed. Loss: 2162517.5000\n",
      "Step 143 of episode 0 completed. Loss: 2157314.0000\n",
      "Step 144 of episode 0 completed. Loss: 2153450.0000\n",
      "Step 145 of episode 0 completed. Loss: 2147927.5000\n",
      "Step 146 of episode 0 completed. Loss: 2142622.0000\n",
      "Step 147 of episode 0 completed. Loss: 2137864.5000\n",
      "Step 148 of episode 0 completed. Loss: 2132724.7500\n",
      "Step 149 of episode 0 completed. Loss: 2126797.7500\n",
      "Step 150 of episode 0 completed. Loss: 2121673.5000\n",
      "Step 151 of episode 0 completed. Loss: 2116495.5000\n",
      "Step 152 of episode 0 completed. Loss: 2110764.2500\n",
      "Step 153 of episode 0 completed. Loss: 2104635.5000\n",
      "Step 154 of episode 0 completed. Loss: 2099614.5000\n",
      "Step 155 of episode 0 completed. Loss: 2093635.2500\n",
      "Step 156 of episode 0 completed. Loss: 2088380.2500\n",
      "Step 157 of episode 0 completed. Loss: 2083063.1250\n",
      "Step 158 of episode 0 completed. Loss: 2077616.5000\n",
      "Step 159 of episode 0 completed. Loss: 2072243.0000\n",
      "Step 160 of episode 0 completed. Loss: 2066849.0000\n",
      "Step 161 of episode 0 completed. Loss: 2061310.7500\n",
      "Step 162 of episode 0 completed. Loss: 2055668.3750\n",
      "Step 163 of episode 0 completed. Loss: 2050045.6250\n",
      "Step 164 of episode 0 completed. Loss: 2044423.0000\n",
      "Step 165 of episode 0 completed. Loss: 2038750.6250\n",
      "Step 166 of episode 0 completed. Loss: 2033063.5000\n",
      "Step 167 of episode 0 completed. Loss: 2027423.5000\n",
      "Step 168 of episode 0 completed. Loss: 2021763.0000\n",
      "Step 169 of episode 0 completed. Loss: 2016065.0000\n",
      "Step 170 of episode 0 completed. Loss: 2010316.2500\n",
      "Step 171 of episode 0 completed. Loss: 2004470.2500\n",
      "Step 172 of episode 0 completed. Loss: 1998513.0000\n",
      "Step 173 of episode 0 completed. Loss: 1992639.1250\n",
      "Step 174 of episode 0 completed. Loss: 1986769.0000\n",
      "Step 175 of episode 0 completed. Loss: 1980868.7500\n",
      "Step 176 of episode 0 completed. Loss: 1974936.0000\n",
      "Step 177 of episode 0 completed. Loss: 1968973.2500\n",
      "Step 178 of episode 0 completed. Loss: 1962978.0000\n",
      "Step 179 of episode 0 completed. Loss: 1956952.0000\n",
      "Step 180 of episode 0 completed. Loss: 1950893.2500\n",
      "Step 181 of episode 0 completed. Loss: 1944803.6250\n",
      "Step 182 of episode 0 completed. Loss: 1938683.0000\n",
      "Step 183 of episode 0 completed. Loss: 1932529.7500\n",
      "Step 184 of episode 0 completed. Loss: 1926346.3750\n",
      "Step 185 of episode 0 completed. Loss: 1920132.1250\n",
      "Step 186 of episode 0 completed. Loss: 1913884.8750\n",
      "Step 187 of episode 0 completed. Loss: 1907606.7500\n",
      "Step 188 of episode 0 completed. Loss: 1901298.7500\n",
      "Step 189 of episode 0 completed. Loss: 1894958.6250\n",
      "Step 190 of episode 0 completed. Loss: 1888585.3750\n",
      "Step 191 of episode 0 completed. Loss: 1882182.8750\n",
      "Step 192 of episode 0 completed. Loss: 1875748.1250\n",
      "Step 193 of episode 0 completed. Loss: 1869281.7500\n",
      "Step 194 of episode 0 completed. Loss: 1862784.0000\n",
      "Step 195 of episode 0 completed. Loss: 1856255.6250\n",
      "Step 196 of episode 0 completed. Loss: 1849695.1250\n",
      "Step 197 of episode 0 completed. Loss: 1843101.2500\n",
      "Step 198 of episode 0 completed. Loss: 1836478.0000\n",
      "Step 199 of episode 0 completed. Loss: 1829820.7500\n",
      "Step 200 of episode 0 completed. Loss: 1823132.8750\n",
      "Step 201 of episode 0 completed. Loss: 1955598.5000\n",
      "Step 202 of episode 0 completed. Loss: 1809698.5000\n",
      "Step 203 of episode 0 completed. Loss: 1802898.5000\n",
      "Step 204 of episode 0 completed. Loss: 1796040.7500\n",
      "Step 205 of episode 0 completed. Loss: 1789164.5000\n",
      "Step 206 of episode 0 completed. Loss: 1782257.0000\n",
      "Step 207 of episode 0 completed. Loss: 1775324.0000\n",
      "Step 208 of episode 0 completed. Loss: 1768368.3750\n",
      "Step 209 of episode 0 completed. Loss: 1761379.1250\n",
      "Step 210 of episode 0 completed. Loss: 1754360.2500\n",
      "Step 211 of episode 0 completed. Loss: 1747320.2500\n",
      "Step 212 of episode 0 completed. Loss: 1740238.7500\n",
      "Step 213 of episode 0 completed. Loss: 1733125.0000\n",
      "Step 214 of episode 0 completed. Loss: 1725983.0000\n",
      "Step 215 of episode 0 completed. Loss: 1718806.2500\n",
      "Step 216 of episode 0 completed. Loss: 1711597.0000\n",
      "Step 217 of episode 0 completed. Loss: 1704347.2500\n",
      "Step 218 of episode 0 completed. Loss: 1697065.7500\n",
      "Step 219 of episode 0 completed. Loss: 1689759.5000\n",
      "Step 220 of episode 0 completed. Loss: 1682438.1250\n",
      "Step 221 of episode 0 completed. Loss: 1675064.7500\n",
      "Step 222 of episode 0 completed. Loss: 1667647.0000\n",
      "Step 223 of episode 0 completed. Loss: 1660204.5000\n",
      "Step 224 of episode 0 completed. Loss: 1652733.7500\n",
      "Step 225 of episode 0 completed. Loss: 1645231.2500\n",
      "Step 226 of episode 0 completed. Loss: 1637707.7500\n",
      "Step 227 of episode 0 completed. Loss: 1630139.0000\n",
      "Step 228 of episode 0 completed. Loss: 1622536.2500\n",
      "Step 229 of episode 0 completed. Loss: 1614912.7500\n",
      "Step 230 of episode 0 completed. Loss: 1607244.3750\n",
      "Step 231 of episode 0 completed. Loss: 1599541.7500\n",
      "Step 232 of episode 0 completed. Loss: 1591819.0000\n",
      "Step 233 of episode 0 completed. Loss: 1584041.2500\n",
      "Step 234 of episode 0 completed. Loss: 1576223.2500\n",
      "Step 235 of episode 0 completed. Loss: 1568381.5000\n",
      "Step 236 of episode 0 completed. Loss: 1560438.7500\n",
      "Step 237 of episode 0 completed. Loss: 1552497.5000\n",
      "Step 238 of episode 0 completed. Loss: 1544547.7500\n",
      "Step 239 of episode 0 completed. Loss: 1536551.5000\n",
      "Step 240 of episode 0 completed. Loss: 1528520.2500\n",
      "Step 241 of episode 0 completed. Loss: 1520464.6250\n",
      "Step 242 of episode 0 completed. Loss: 1512364.2500\n",
      "Step 243 of episode 0 completed. Loss: 1504229.2500\n",
      "Step 244 of episode 0 completed. Loss: 1496068.3750\n",
      "Step 245 of episode 0 completed. Loss: 1487865.6250\n",
      "Step 246 of episode 0 completed. Loss: 1479628.7500\n",
      "Step 247 of episode 0 completed. Loss: 1471365.5000\n",
      "Step 248 of episode 0 completed. Loss: 1463062.5000\n",
      "Step 249 of episode 0 completed. Loss: 1454724.7500\n",
      "Step 250 of episode 0 completed. Loss: 1446354.0000\n",
      "Step 251 of episode 0 completed. Loss: 1437950.2500\n",
      "Step 252 of episode 0 completed. Loss: 1429511.7500\n",
      "Step 253 of episode 0 completed. Loss: 1421042.2500\n",
      "Step 254 of episode 0 completed. Loss: 1412538.7500\n",
      "Step 255 of episode 0 completed. Loss: 1404003.0000\n",
      "Step 256 of episode 0 completed. Loss: 1395434.0000\n",
      "Step 257 of episode 0 completed. Loss: 1386831.7500\n",
      "Step 258 of episode 0 completed. Loss: 1378196.2500\n",
      "Step 259 of episode 0 completed. Loss: 1369530.7500\n",
      "Step 260 of episode 0 completed. Loss: 1360829.6250\n",
      "Step 261 of episode 0 completed. Loss: 1352094.7500\n",
      "Step 262 of episode 0 completed. Loss: 1343329.7500\n",
      "Step 263 of episode 0 completed. Loss: 1334528.2500\n",
      "Step 264 of episode 0 completed. Loss: 1325694.7500\n",
      "Step 265 of episode 0 completed. Loss: 1316830.5000\n",
      "Step 266 of episode 0 completed. Loss: 1307931.5000\n",
      "Step 267 of episode 0 completed. Loss: 1298998.5000\n",
      "Step 268 of episode 0 completed. Loss: 1290033.5000\n",
      "Step 269 of episode 0 completed. Loss: 1281034.2500\n",
      "Step 270 of episode 0 completed. Loss: 1272001.2500\n",
      "Step 271 of episode 0 completed. Loss: 1262937.0000\n",
      "Step 272 of episode 0 completed. Loss: 1253838.2500\n",
      "Step 273 of episode 0 completed. Loss: 1244705.8750\n",
      "Step 274 of episode 0 completed. Loss: 1235540.7500\n",
      "Step 275 of episode 0 completed. Loss: 1226341.5000\n",
      "Step 276 of episode 0 completed. Loss: 1217109.5000\n",
      "Step 277 of episode 0 completed. Loss: 1207844.0000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_, agent \u001b[38;5;129;01min\u001b[39;00m players\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     45\u001b[0m     network_outs[id_] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact_train(step\u001b[38;5;241m=\u001b[39mstep, obs\u001b[38;5;241m=\u001b[39mobs[id_])\n\u001b[0;32m---> 47\u001b[0m     actions[id_] \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# save actions\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     last_actions[id_] \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Desktop/School/Neuroscience/RL_Textbook/lux_s3_submission/agent.py:175\u001b[0m, in \u001b[0;36mAgent.sample_actions\u001b[0;34m(self, action_probs, sap_offset)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Sample the discrete action\u001b[39;00m\n\u001b[1;32m    174\u001b[0m _, batch_size, action_space \u001b[38;5;241m=\u001b[39m action_probs\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 175\u001b[0m action_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size,)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Initialize output dx and dy\u001b[39;00m\n\u001b[1;32m    179\u001b[0m sampled_dx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# set some hyperparams\n",
    "episode_num = 0\n",
    "reward_history = []\n",
    "network_difs = []\n",
    "wins = 0\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "value_coeff=0.5\n",
    "entropy_coeff=0.01\n",
    "win_rates = []\n",
    "\n",
    "while True:\n",
    "    obs, info = env.reset()\n",
    "    game_done = False\n",
    "    wins = 0\n",
    "    step = 0\n",
    "    last_obs = {}\n",
    "    last_actions = {}\n",
    "    print(f\"episode num: {episode_num}\")\n",
    "\n",
    "    # initialize rewards array and trajectories\n",
    "    rewards = {\n",
    "        \"player_0\": [],\n",
    "        \"player_1\": []\n",
    "    }\n",
    "\n",
    "    # save last env reward \n",
    "    last_env_reward = {\n",
    "        \"player_0\": np.zeros(1, dtype=np.int32),\n",
    "        \"player_1\": np.zeros(1, dtype=np.int32)\n",
    "    }\n",
    "    \n",
    "    while not game_done:\n",
    "        actions = {}\n",
    "        # store current observations for learning\n",
    "        last_obs = {\n",
    "            \"player_0\": obs[\"player_0\"].copy(),\n",
    "            \"player_1\": obs[\"player_1\"].copy()\n",
    "        }\n",
    "\n",
    "        # get network output, including actions\n",
    "        network_outs = {}\n",
    "        for id_, agent in players.items():\n",
    "            \n",
    "            network_outs[id_] = agent.act_train(step=step, obs=obs[id_])\n",
    "\n",
    "            actions[id_] = agent.sample_actions(network_outs[id_][1], network_outs[id_][2])\n",
    "\n",
    "            # save actions\n",
    "            last_actions[id_] = actions.copy()\n",
    "\n",
    "         \n",
    "        # step in environment for both agents\n",
    "        obs, reward, terminated, truncated, info = env.step(actions)\n",
    "        match_result = None\n",
    "        if (last_env_reward != reward):\n",
    "            if reward[\"player_0\"] > last_env_reward[\"player_0\"]:\n",
    "                match_result = \"win\"\n",
    "            elif reward[\"player_1\"] > last_env_reward[\"player_1\"]:\n",
    "                match_result = \"loss\"\n",
    "            else:\n",
    "                match_result = \"draw\"\n",
    "\n",
    "        last_env_reward = reward.copy()\n",
    "\n",
    "        # calc rewards for both agents\n",
    "        for id_, agent in players.items():\n",
    "            map_memory, enemy_memory, ally_memory, relic_points, _, _ = agent.process_obs(obs[id_])\n",
    "            rewards[id_].append(calculate_rewards(network_outs[id_][0].squeeze(0).detach().cpu().numpy(), map_memory, enemy_memory, ally_memory, relic_points, match_result))\n",
    "            \n",
    "\n",
    "        # calc whether game is finished\n",
    "        dones = {k: terminated[k] | truncated[k] for k in terminated}\n",
    "\n",
    "        # Compute returns and advantages for player 0\n",
    "        returns, advantages = compute_advantages(\n",
    "            rewards=[rewards[\"player_0\"][-1]],\n",
    "            values=[network_outs[\"player_0\"][3].squeeze(0).squeeze(-1).detach().cpu().numpy()],\n",
    "            gamma=gamma,\n",
    "            lambda_=lambda_\n",
    "        )\n",
    "\n",
    "        # compute losses\n",
    "        weight_loss = compute_weight_loss(\n",
    "            log_probs=torch.cat((network_outs[\"player_0\"][1].log(), network_outs[\"player_0\"][2].log()), dim=-1).to(device),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "        action_loss = compute_action_loss(\n",
    "            log_probs=network_outs[\"player_0\"][1].log(),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "    \n",
    "        # backpropogation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = weight_loss + action_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Step {step} of episode {episode_num} completed. Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        if dones[\"player_0\"] or dones[\"player_1\"]:\n",
    "            game_done = True\n",
    "            # save model weights\n",
    "            torch.save(players[\"player_0\"].net.state_dict(), f\"models/agent_network_episode_{episode_num}\")\n",
    "            print(reward)\n",
    "            wins += reward\n",
    "            win_rates.append(wins / num_episodes)\n",
    "        step += 1\n",
    "\n",
    "    # store rewards\n",
    "    reward_history.append(rewards[\"player_0\"])\n",
    "   \n",
    "\n",
    "    # calc l2 norm\n",
    "    network_dif = compute_network_difference(players[\"player_0\"].net, player[\"players_1\"].net)\n",
    "    network_difs.append(network_dif)\n",
    "\n",
    "    # update adversary to current state dict every 5 episodes\n",
    "    if episode_num % 5 == 0:\n",
    "        players[\"player_1\"].net.load_state_dict(players[\"player_0\"].net.state_dict())\n",
    "\n",
    "    # if network has converged according to our criterion break out of training loop\n",
    "    if has_converged(win_rates, network_difs):\n",
    "        break\n",
    "        \n",
    "    episode_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679f8e2-5a1a-479c-9d9c-f4c1296f2b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a2aa4-65aa-4b98-b4ac-f21f09918777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (lux_env)",
   "language": "python",
   "name": "lux_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
