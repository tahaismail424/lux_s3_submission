{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465149cd-f249-4d74-8006-c9c1aea35e8e",
   "metadata": {},
   "source": [
    "### this notebook is for training our Agent in the lux environment and see how well we do :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fef873-ccc1-4ce2-bb74-d5a74cee1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from agent import Agent\n",
    "from network import AgentNetwork, compute_network_difference, has_converged\n",
    "from rewards import calculate_rewards\n",
    "from ac2methods import compute_advantages, compute_weight_loss, compute_action_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929f67f5-870e-4fa9-a0e1-47086019e6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-01-25 19:15:41,412:jax._src.xla_bridge:987: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# reset our gym environment\n",
    "env = LuxAIS3GymEnv(numpy_output=True)\n",
    "obs, info = env.reset()\n",
    "\n",
    "env_cfg = info[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c346dbe-184e-44ea-86c9-36f360152951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cf25d8-8d9e-462e-bacc-8bf36350b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set players\n",
    "players = {\n",
    "    \"player_0\": Agent(\"player_0\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6).to(device), device),\n",
    "    \"player_1\": Agent(\"player_1\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6).to(device), device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ecc61c-0b05-4fdc-9ba7-895cc461d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer for network\n",
    "optimizer = torch.optim.Adam(players[\"player_0\"].net.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c9da1-b84f-4932-b22c-20124a7ca50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode num: 0\n",
      "Step 0 of episode 0 completed. Loss: 4091688.0000\n",
      "Step 1 of episode 0 completed. Loss: 4079193.7500\n",
      "Step 2 of episode 0 completed. Loss: 4055911.0000\n",
      "Step 3 of episode 0 completed. Loss: 4044548.2500\n",
      "Step 4 of episode 0 completed. Loss: 4030434.7500\n",
      "Step 5 of episode 0 completed. Loss: 4030691.0000\n",
      "Step 6 of episode 0 completed. Loss: 4042333.2500\n",
      "Step 7 of episode 0 completed. Loss: 4058037.0000\n",
      "Step 8 of episode 0 completed. Loss: 4045296.5000\n",
      "Step 9 of episode 0 completed. Loss: 4089128.5000\n",
      "Step 10 of episode 0 completed. Loss: 4168543.5000\n",
      "Step 11 of episode 0 completed. Loss: 4129852.2500\n",
      "Step 12 of episode 0 completed. Loss: 4084743.0000\n",
      "Step 13 of episode 0 completed. Loss: 4023290.7500\n",
      "Step 14 of episode 0 completed. Loss: 4002421.7500\n",
      "Step 15 of episode 0 completed. Loss: 3965528.0000\n",
      "Step 16 of episode 0 completed. Loss: 3899229.0000\n",
      "Step 17 of episode 0 completed. Loss: 3801881.0000\n",
      "Step 18 of episode 0 completed. Loss: 3624131.0000\n",
      "Step 19 of episode 0 completed. Loss: 3563101.5000\n",
      "Step 20 of episode 0 completed. Loss: 3389884.2500\n",
      "Step 21 of episode 0 completed. Loss: 3327593.0000\n",
      "Step 22 of episode 0 completed. Loss: 3325873.0000\n",
      "Step 23 of episode 0 completed. Loss: 3227262.0000\n",
      "Step 24 of episode 0 completed. Loss: 3191766.0000\n",
      "Step 25 of episode 0 completed. Loss: 3115362.0000\n",
      "Step 26 of episode 0 completed. Loss: 2970891.0000\n",
      "Step 27 of episode 0 completed. Loss: 2849343.0000\n",
      "Step 28 of episode 0 completed. Loss: 2819813.0000\n",
      "Step 29 of episode 0 completed. Loss: 2771851.5000\n",
      "Step 30 of episode 0 completed. Loss: 2702819.5000\n",
      "Step 31 of episode 0 completed. Loss: 2682308.5000\n",
      "Step 32 of episode 0 completed. Loss: 2690718.0000\n",
      "Step 33 of episode 0 completed. Loss: 2692733.5000\n",
      "Step 34 of episode 0 completed. Loss: 2728308.2500\n",
      "Step 35 of episode 0 completed. Loss: 2643377.0000\n",
      "Step 36 of episode 0 completed. Loss: 2645075.0000\n",
      "Step 37 of episode 0 completed. Loss: 2627050.5000\n",
      "Step 38 of episode 0 completed. Loss: 2592426.5000\n",
      "Step 39 of episode 0 completed. Loss: 2594752.7500\n",
      "Step 40 of episode 0 completed. Loss: 2575732.7500\n",
      "Step 41 of episode 0 completed. Loss: 2571298.0000\n",
      "Step 42 of episode 0 completed. Loss: 2538226.5000\n",
      "Step 43 of episode 0 completed. Loss: 2505885.5000\n",
      "Step 44 of episode 0 completed. Loss: 2529513.2500\n",
      "Step 45 of episode 0 completed. Loss: 2565997.0000\n",
      "Step 46 of episode 0 completed. Loss: 2589351.5000\n",
      "Step 47 of episode 0 completed. Loss: 2631959.0000\n",
      "Step 48 of episode 0 completed. Loss: 2675310.0000\n",
      "Step 49 of episode 0 completed. Loss: 2649181.5000\n",
      "Step 50 of episode 0 completed. Loss: 2671029.5000\n",
      "Step 51 of episode 0 completed. Loss: 2721986.0000\n",
      "Step 52 of episode 0 completed. Loss: 2700714.5000\n",
      "Step 53 of episode 0 completed. Loss: 2717984.0000\n",
      "Step 54 of episode 0 completed. Loss: 2761899.7500\n",
      "Step 55 of episode 0 completed. Loss: 2736377.0000\n",
      "Step 56 of episode 0 completed. Loss: 2752124.0000\n",
      "Step 57 of episode 0 completed. Loss: 2772419.5000\n",
      "Step 58 of episode 0 completed. Loss: 2763715.0000\n",
      "Step 59 of episode 0 completed. Loss: 2718939.7500\n",
      "Step 60 of episode 0 completed. Loss: 2673910.5000\n",
      "Step 61 of episode 0 completed. Loss: 2644473.0000\n",
      "Step 62 of episode 0 completed. Loss: 2595641.5000\n",
      "Step 63 of episode 0 completed. Loss: 2574883.5000\n",
      "Step 64 of episode 0 completed. Loss: 2569575.7500\n",
      "Step 65 of episode 0 completed. Loss: 2538568.0000\n",
      "Step 66 of episode 0 completed. Loss: 2519071.2500\n",
      "Step 67 of episode 0 completed. Loss: 2503543.5000\n",
      "Step 68 of episode 0 completed. Loss: 2558738.2500\n",
      "Step 69 of episode 0 completed. Loss: 2554752.0000\n",
      "Step 70 of episode 0 completed. Loss: 2505988.5000\n",
      "Step 71 of episode 0 completed. Loss: 2476189.5000\n",
      "Step 72 of episode 0 completed. Loss: 2476435.7500\n",
      "Step 73 of episode 0 completed. Loss: 2454037.0000\n",
      "Step 74 of episode 0 completed. Loss: 2441947.0000\n",
      "Step 75 of episode 0 completed. Loss: 2435178.5000\n",
      "Step 76 of episode 0 completed. Loss: 2444948.0000\n",
      "Step 77 of episode 0 completed. Loss: 2445670.5000\n",
      "Step 78 of episode 0 completed. Loss: 2442169.5000\n",
      "Step 79 of episode 0 completed. Loss: 2428136.0000\n",
      "Step 80 of episode 0 completed. Loss: 2401824.0000\n",
      "Step 81 of episode 0 completed. Loss: 2374020.5000\n",
      "Step 82 of episode 0 completed. Loss: 2376030.2500\n",
      "Step 83 of episode 0 completed. Loss: 2376817.5000\n",
      "Step 84 of episode 0 completed. Loss: 2368786.5000\n",
      "Step 85 of episode 0 completed. Loss: 2361450.0000\n",
      "Step 86 of episode 0 completed. Loss: 2355077.0000\n",
      "Step 87 of episode 0 completed. Loss: 2352232.5000\n",
      "Step 88 of episode 0 completed. Loss: 2348923.5000\n",
      "Step 89 of episode 0 completed. Loss: 2346704.5000\n",
      "Step 90 of episode 0 completed. Loss: 2345451.5000\n",
      "Step 91 of episode 0 completed. Loss: 2348126.0000\n",
      "Step 92 of episode 0 completed. Loss: 2340829.7500\n",
      "Step 93 of episode 0 completed. Loss: 2327748.5000\n",
      "Step 94 of episode 0 completed. Loss: 2322087.5000\n",
      "Step 95 of episode 0 completed. Loss: 2316570.0000\n",
      "Step 96 of episode 0 completed. Loss: 2308452.7500\n",
      "Step 97 of episode 0 completed. Loss: 2299964.0000\n",
      "Step 98 of episode 0 completed. Loss: 2294717.7500\n",
      "Step 99 of episode 0 completed. Loss: 2291724.5000\n",
      "Step 100 of episode 0 completed. Loss: 1997883.5000\n",
      "Step 101 of episode 0 completed. Loss: 2259844.5000\n",
      "Step 102 of episode 0 completed. Loss: 2295859.0000\n",
      "Step 103 of episode 0 completed. Loss: 2265971.0000\n",
      "Step 104 of episode 0 completed. Loss: 2252127.5000\n",
      "Step 105 of episode 0 completed. Loss: 2378916.0000\n",
      "Step 106 of episode 0 completed. Loss: 2337413.5000\n",
      "Step 107 of episode 0 completed. Loss: 2278002.5000\n",
      "Step 108 of episode 0 completed. Loss: 2313805.5000\n",
      "Step 109 of episode 0 completed. Loss: 2247429.2500\n",
      "Step 110 of episode 0 completed. Loss: 2195015.0000\n",
      "Step 111 of episode 0 completed. Loss: 2121351.0000\n",
      "Step 112 of episode 0 completed. Loss: 2061676.5000\n",
      "Step 113 of episode 0 completed. Loss: 1988547.7500\n",
      "Step 114 of episode 0 completed. Loss: 2086948.7500\n",
      "Step 115 of episode 0 completed. Loss: 2062363.7500\n",
      "Step 116 of episode 0 completed. Loss: 2031692.0000\n",
      "Step 117 of episode 0 completed. Loss: 2190405.0000\n",
      "Step 118 of episode 0 completed. Loss: 2158210.0000\n",
      "Step 119 of episode 0 completed. Loss: 2126322.5000\n",
      "Step 120 of episode 0 completed. Loss: 2237726.5000\n",
      "Step 121 of episode 0 completed. Loss: 2180726.5000\n",
      "Step 122 of episode 0 completed. Loss: 2129856.5000\n",
      "Step 123 of episode 0 completed. Loss: 1985843.2500\n",
      "Step 124 of episode 0 completed. Loss: 2015652.0000\n",
      "Step 125 of episode 0 completed. Loss: 2039026.6250\n",
      "Step 126 of episode 0 completed. Loss: 1853538.6250\n",
      "Step 127 of episode 0 completed. Loss: 1881549.7500\n",
      "Step 128 of episode 0 completed. Loss: 1872508.8750\n",
      "Step 129 of episode 0 completed. Loss: 1699497.6250\n",
      "Step 130 of episode 0 completed. Loss: 1716269.3750\n",
      "Step 131 of episode 0 completed. Loss: 1737092.7500\n",
      "Step 132 of episode 0 completed. Loss: 1702568.5000\n",
      "Step 133 of episode 0 completed. Loss: 1738079.5000\n",
      "Step 134 of episode 0 completed. Loss: 1771774.7500\n",
      "Step 135 of episode 0 completed. Loss: 1737934.0000\n",
      "Step 136 of episode 0 completed. Loss: 1762821.5000\n",
      "Step 137 of episode 0 completed. Loss: 1807655.7500\n",
      "Step 138 of episode 0 completed. Loss: 1835430.2500\n",
      "Step 139 of episode 0 completed. Loss: 1826611.5000\n"
     ]
    }
   ],
   "source": [
    "# set some hyperparams\n",
    "episode_num = 0\n",
    "reward_history = []\n",
    "network_difs = []\n",
    "wins = 0\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "value_coeff=0.5\n",
    "entropy_coeff=0.01\n",
    "win_rates = []\n",
    "\n",
    "while True:\n",
    "    obs, info = env.reset()\n",
    "    game_done = False\n",
    "    wins = 0\n",
    "    step = 0\n",
    "    last_obs = {}\n",
    "    last_actions = {}\n",
    "    print(f\"episode num: {episode_num}\")\n",
    "\n",
    "    # initialize rewards array and trajectories\n",
    "    rewards = {\n",
    "        \"player_0\": [],\n",
    "        \"player_1\": []\n",
    "    }\n",
    "\n",
    "    # save last env reward \n",
    "    last_env_reward = {\n",
    "        \"player_0\": np.zeros(1, dtype=np.int32),\n",
    "        \"player_1\": np.zeros(1, dtype=np.int32)\n",
    "    }\n",
    "    \n",
    "    while not game_done:\n",
    "        actions = {}\n",
    "        # store current observations for learning\n",
    "        last_obs = {\n",
    "            \"player_0\": obs[\"player_0\"].copy(),\n",
    "            \"player_1\": obs[\"player_1\"].copy()\n",
    "        }\n",
    "\n",
    "        # get network output, including actions\n",
    "        network_outs = {}\n",
    "        for id_, agent in players.items():\n",
    "            \n",
    "            network_outs[id_] = agent.act_train(step=step, obs=obs[id_])\n",
    "\n",
    "            actions[id_] = agent.sample_actions(network_outs[id_][1].detach().cpu(), network_outs[id_][2].detach().cpu())\n",
    "\n",
    "            # save actions\n",
    "            last_actions[id_] = actions.copy()\n",
    "\n",
    "         \n",
    "        # step in environment for both agents\n",
    "        obs, reward, terminated, truncated, info = env.step(actions)\n",
    "        match_result = None\n",
    "        if (last_env_reward != reward):\n",
    "            if reward[\"player_0\"] > last_env_reward[\"player_0\"]:\n",
    "                match_result = \"win\"\n",
    "            elif reward[\"player_1\"] > last_env_reward[\"player_1\"]:\n",
    "                match_result = \"loss\"\n",
    "            else:\n",
    "                match_result = \"draw\"\n",
    "\n",
    "        last_env_reward = reward.copy()\n",
    "\n",
    "        # calc rewards for both agents\n",
    "        for id_, agent in players.items():\n",
    "            map_memory, enemy_memory, ally_memory, relic_points, _, _ = agent.process_obs(obs[id_])\n",
    "            rewards[id_].append(calculate_rewards(network_outs[id_][0].squeeze(0).detach().cpu().numpy(), map_memory, enemy_memory, ally_memory, relic_points, match_result))\n",
    "            \n",
    "\n",
    "        # calc whether game is finished\n",
    "        dones = {k: terminated[k] | truncated[k] for k in terminated}\n",
    "\n",
    "        # Compute returns and advantages for player 0\n",
    "        returns, advantages = compute_advantages(\n",
    "            rewards=[rewards[\"player_0\"][-1]],\n",
    "            values=[network_outs[\"player_0\"][3].squeeze(0).squeeze(-1).detach().cpu().numpy()],\n",
    "            gamma=gamma,\n",
    "            lambda_=lambda_\n",
    "        )\n",
    "\n",
    "        # compute losses\n",
    "        weight_loss = compute_weight_loss(\n",
    "            log_probs=torch.cat((network_outs[\"player_0\"][1].log(), network_outs[\"player_0\"][2].log()), dim=-1).to(device),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "        action_loss = compute_action_loss(\n",
    "            log_probs=network_outs[\"player_0\"][1].log(),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "    \n",
    "        # backpropogation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = weight_loss + action_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Step {step} of episode {episode_num} completed. Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        if dones[\"player_0\"] or dones[\"player_1\"]:\n",
    "            game_done = True\n",
    "            # save model weights\n",
    "            torch.save(players[\"player_0\"].net.state_dict(), f\"models/agent_network_episode_{episode_num}\")\n",
    "            wins += int(reward[\"player_0\"] > reward[\"player_1\"])\n",
    "            print(wins)\n",
    "            win_rates.append(wins / (episode_num + 1))\n",
    "        step += 1\n",
    "\n",
    "    # store rewards\n",
    "    reward_history.append(rewards[\"player_0\"])\n",
    "   \n",
    "\n",
    "    # calc l2 norm\n",
    "    network_dif = compute_network_difference(players[\"player_0\"].net, players[\"player_1\"].net)\n",
    "    network_difs.append(network_dif)\n",
    "\n",
    "    # update adversary to current state dict every 5 episodes\n",
    "    if episode_num % 5 == 0:\n",
    "        players[\"player_1\"].net.load_state_dict(players[\"player_0\"].net.state_dict())\n",
    "\n",
    "    # if network has converged according to our criterion break out of training loop\n",
    "    if has_converged(win_rates, network_difs):\n",
    "        break\n",
    "        \n",
    "    episode_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679f8e2-5a1a-479c-9d9c-f4c1296f2b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a2aa4-65aa-4b98-b4ac-f21f09918777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db2411-e81f-47f9-91a9-b0615f95e531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b712eb-f068-4be5-934d-60f32053a790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
