{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465149cd-f249-4d74-8006-c9c1aea35e8e",
   "metadata": {},
   "source": [
    "### this notebook is for training our Agent in the lux environment and see how well we do :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fef873-ccc1-4ce2-bb74-d5a74cee1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from agent import Agent\n",
    "from network import AgentNetwork, compute_network_difference, has_converged\n",
    "from rewards import calculate_rewards\n",
    "from ac2methods import compute_advantages, compute_weight_loss, compute_action_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929f67f5-870e-4fa9-a0e1-47086019e6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-01-24 04:22:07,730:jax._src.xla_bridge:987: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# reset our gym environment\n",
    "env = LuxAIS3GymEnv(numpy_output=True)\n",
    "obs, info = env.reset()\n",
    "\n",
    "env_cfg = info[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c346dbe-184e-44ea-86c9-36f360152951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cf25d8-8d9e-462e-bacc-8bf36350b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set players\n",
    "players = {\n",
    "    \"player_0\": Agent(\"player_0\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6).to(device), device),\n",
    "    \"player_1\": Agent(\"player_1\", env_cfg, AgentNetwork((env_cfg[\"map_width\"], env_cfg[\"map_height\"]), env_cfg[\"max_units\"], 6).to(device), device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ecc61c-0b05-4fdc-9ba7-895cc461d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer for network\n",
    "optimizer = torch.optim.Adam(players[\"player_0\"].net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906c9da1-b84f-4932-b22c-20124a7ca50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode num: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m network_outs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id_, agent \u001b[38;5;129;01min\u001b[39;00m players\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 45\u001b[0m     network_outs[id_] \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     actions[id_] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msample_actions(network_outs[id_][\u001b[38;5;241m1\u001b[39m], network_outs[id_][\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# save actions\u001b[39;00m\n",
      "File \u001b[0;32m~/lux_s3_submission/agent.py:55\u001b[0m, in \u001b[0;36mAgent.act_train\u001b[0;34m(self, step, obs, remainingOverageTime)\u001b[0m\n\u001b[1;32m     45\u001b[0m shared_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(map_memory, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menemy_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(enemy_memory, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msap_range\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(sap_range, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     52\u001b[0m }\n\u001b[1;32m     54\u001b[0m ship_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallied_memory, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mship_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/lux_s3_submission/network.py:142\u001b[0m, in \u001b[0;36mAgentNetwork.forward\u001b[0;34m(self, shared_inputs, ship_states, hidden_state)\u001b[0m\n\u001b[1;32m    139\u001b[0m weights_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_fc(weights_out)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# process sap range\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m sap_range_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msap_range_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msap_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, ship_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# compute action policy\u001b[39;00m\n\u001b[1;32m    145\u001b[0m combined_action_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rnn_out, weights_features, sap_range_features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# set some hyperparams\n",
    "episode_num = 0\n",
    "reward_history = []\n",
    "network_difs = []\n",
    "wins = 0\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "value_coeff=0.5\n",
    "entropy_coeff=0.01\n",
    "win_rates = []\n",
    "\n",
    "while True:\n",
    "    obs, info = env.reset()\n",
    "    game_done = False\n",
    "    wins = 0\n",
    "    step = 0\n",
    "    last_obs = {}\n",
    "    last_actions = {}\n",
    "    print(f\"episode num: {episode_num}\")\n",
    "\n",
    "    # initialize rewards array and trajectories\n",
    "    rewards = {\n",
    "        \"player_0\": [],\n",
    "        \"player_1\": []\n",
    "    }\n",
    "\n",
    "    # save last env reward \n",
    "    last_env_reward = {\n",
    "        \"player_0\": np.zeros(1, dtype=np.int32),\n",
    "        \"player_1\": np.zeros(1, dtype=np.int32)\n",
    "    }\n",
    "    \n",
    "    while not game_done:\n",
    "        actions = {}\n",
    "        # store current observations for learning\n",
    "        last_obs = {\n",
    "            \"player_0\": obs[\"player_0\"].copy(),\n",
    "            \"player_1\": obs[\"player_1\"].copy()\n",
    "        }\n",
    "\n",
    "        # get network output, including actions\n",
    "        network_outs = {}\n",
    "        for id_, agent in players.items():\n",
    "            \n",
    "            network_outs[id_] = agent.act_train(step=step, obs=obs[id_])\n",
    "\n",
    "            actions[id_] = agent.sample_actions(network_outs[id_][1], network_outs[id_][2])\n",
    "\n",
    "            # save actions\n",
    "            last_actions[id_] = actions.copy()\n",
    "\n",
    "         \n",
    "        # step in environment for both agents\n",
    "        obs, reward, terminated, truncated, info = env.step(actions)\n",
    "        match_result = None\n",
    "        if (last_env_reward != reward):\n",
    "            if reward[\"player_0\"] > last_env_reward[\"player_0\"]:\n",
    "                match_result = \"win\"\n",
    "            elif reward[\"player_1\"] > last_env_reward[\"player_1\"]:\n",
    "                match_result = \"loss\"\n",
    "            else:\n",
    "                match_result = \"draw\"\n",
    "\n",
    "        last_env_reward = reward.copy()\n",
    "\n",
    "        # calc rewards for both agents\n",
    "        for id_, agent in players.items():\n",
    "            map_memory, enemy_memory, ally_memory, relic_points, _, _ = agent.process_obs(obs[id_])\n",
    "            rewards[id_].append(calculate_rewards(network_outs[id_][0].squeeze(0).detach().cpu().numpy(), map_memory, enemy_memory, ally_memory, relic_points, match_result))\n",
    "            \n",
    "\n",
    "        # calc whether game is finished\n",
    "        dones = {k: terminated[k] | truncated[k] for k in terminated}\n",
    "\n",
    "        # Compute returns and advantages for player 0\n",
    "        returns, advantages = compute_advantages(\n",
    "            rewards=[rewards[\"player_0\"][-1]],\n",
    "            values=[network_outs[\"player_0\"][3].squeeze(0).squeeze(-1).detach().cpu().numpy()],\n",
    "            gamma=gamma,\n",
    "            lambda_=lambda_\n",
    "        )\n",
    "\n",
    "        # compute losses\n",
    "        weight_loss = compute_weight_loss(\n",
    "            log_probs=torch.cat((network_outs[\"player_0\"][1].log(), network_outs[\"player_0\"][2].log()), dim=-1).to(device),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "        action_loss = compute_action_loss(\n",
    "            log_probs=network_outs[\"player_0\"][1].log(),\n",
    "            advantages=torch.tensor(advantages, dtype=torch.float32).to(device),\n",
    "            values=network_outs[\"player_0\"][3].squeeze(-1),\n",
    "            returns=torch.tensor(returns, dtype=torch.float32).to(device),\n",
    "            entropy_coeff=entropy_coeff,\n",
    "            value_coeff=value_coeff\n",
    "        )\n",
    "    \n",
    "        # backpropogation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = weight_loss + action_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Step {step} of episode {episode_num} completed. Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        if dones[\"player_0\"] or dones[\"player_1\"]:\n",
    "            game_done = True\n",
    "            # save model weights\n",
    "            torch.save(players[\"player_0\"].net.state_dict(), f\"models/agent_network_episode_{episode_num}\")\n",
    "            print(reward)\n",
    "            wins += reward\n",
    "            win_rates.append(wins / num_episodes)\n",
    "        step += 1\n",
    "\n",
    "    # store rewards\n",
    "    reward_history.append(rewards[\"player_0\"])\n",
    "   \n",
    "\n",
    "    # calc l2 norm\n",
    "    network_dif = compute_network_difference(players[\"player_0\"].net, player[\"players_1\"].net)\n",
    "    network_difs.append(network_dif)\n",
    "\n",
    "    # update adversary to current state dict every 5 episodes\n",
    "    if episode_num % 5 == 0:\n",
    "        players[\"player_1\"].net.load_state_dict(players[\"player_0\"].net.state_dict())\n",
    "\n",
    "    # if network has converged according to our criterion break out of training loop\n",
    "    if has_converged(win_rates, network_difs):\n",
    "        break\n",
    "        \n",
    "    episode_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679f8e2-5a1a-479c-9d9c-f4c1296f2b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a2aa4-65aa-4b98-b4ac-f21f09918777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
